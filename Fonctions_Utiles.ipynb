{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as mpl\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import raytracing as rt\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "import scipy.optimize as spo\n",
    "import scipy.constants as spc\n",
    "import scipy.integrate as spi\n",
    "import scipy.linalg as spl\n",
    "import scipy.signal as spsi\n",
    "import scipy.stats as spst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mpl.plot(a=None, b=None,color=\"grey\", label=\"Modèle ajusté de la polarisation TE\")\n",
    "# mpl.errorbar(error_a=None, error_b=None, yerr=11.1, xerr=0.02,fmt='o', color=\"grey\", capsize=3, label=\"Données expérimentales de polarisation TE\")\n",
    "\n",
    "# mpl.ylabel(\"Intensité [µA]\")\n",
    "# mpl.xlabel(\"Angle du rayon incident [°]\")\n",
    "# mpl.legend(frameon=False) \n",
    "# mpl.tick_params(axis='both', which='both', direction='in')\n",
    "# mpl.minorticks_on()\n",
    "\n",
    "# mpl.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour calculer une moyenne\n",
    "def moyenne_2D(liste, n, m):\n",
    "    moyenne = []\n",
    "    sous_moyenne = 0\n",
    "    for i in range(n) :\n",
    "        for j in range(m) :\n",
    "            sous_moyenne += liste[i][j]\n",
    "        moyenne += [sous_moyenne/(m)]\n",
    "        sous_moyenne = 0\n",
    "    return moyenne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "curve_fit() missing 3 required positional arguments: 'f', 'xdata', and 'ydata'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m popt_data, pcov_data \u001b[38;5;241m=\u001b[39m \u001b[43mspo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurve_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfonction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariable_1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariable_2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m perr_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(np\u001b[38;5;241m.\u001b[39mdiag(pcov_data))\n",
      "\u001b[1;31mTypeError\u001b[0m: curve_fit() missing 3 required positional arguments: 'f', 'xdata', and 'ydata'"
     ]
    }
   ],
   "source": [
    "popt_data, pcov_data = spo.curve_fit(fonction=None, variable_1=None, variable_2=None)\n",
    "perr_data = np.sqrt(np.diag(pcov_data))\n",
    "\n",
    "# Définir la fonction gaussienne\n",
    "def gaussienne(x, a, b, c):\n",
    "    return a * np.exp(-(x - b)**2 / (2 * c**2))\n",
    "# Ajustement de la courbe gaussienne aux données\n",
    "popt, pcov = spo.curve_fit(gaussienne, x_data=None, y_data=None, \n",
    "                           p0=[max(y_data=None), np.mean(x_data=None), np.std(x_data=None)])\n",
    "\n",
    "\n",
    "def poisson_distribution(k, lambd):\n",
    "    return spst.poisson.pmf(k, lambd)\n",
    "# Ajustez le modèle de distribution de Poisson aux données\n",
    "params, _ = spo.curve_fit(poisson_distribution, valeurs=None, counts=None, p0=[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(path):\n",
    "    \"\"\"Fonction qui lit les données présentes dans des fichiers.\n",
    "\n",
    "    Args:\n",
    "        path (folder): Le path du dossier où sont les fichiers à lire.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: un dataframe de toutes les données avec une colonne par fichier.\n",
    "    \"\"\"\n",
    "    all_files = glob.glob(os.path.join(path))\n",
    "    dfs = pd.DataFrame()\n",
    "    i = 0\n",
    "    for f in all_files:\n",
    "        dfs[i] = pd.read_csv(f)\n",
    "        i += 1\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(path):\n",
    "    \"\"\"Fonction qui sort les données voulues de fichier \".txt\".\n",
    "\n",
    "    Args:\n",
    "        path (path): chemin d'accès au dossier qui contient les fichiers.\n",
    "\n",
    "    Returns:\n",
    "        array: un array de dataframe contenant les données voulues.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    # Parcourir les fichiers dans le dossier\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            filepath = os.path.join(path, filename)\n",
    "        \n",
    "            # Ouvrir le fichier et lire les données\n",
    "            with open(filepath, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                start_index = lines.index(\">>>>>Begin Spectral Data<<<<<\\n\") + 1\n",
    "                spectral_data = [line.strip().split('\\t') for line in lines[start_index:]]\n",
    "                spectral_data = pd.DataFrame(spectral_data, columns=[0, 1])\n",
    "                # Conversion des valeurs en float\n",
    "                spectral_data[0] = spectral_data[0].str.replace(',', '.').astype(float)\n",
    "                spectral_data[1] = spectral_data[1].str.replace(',', '.').astype(float)\n",
    "                spectral_data[\"Filename\"] = filename\n",
    "                data.append(spectral_data)\n",
    "\n",
    "    # Concaténer tous les DataFrames en un seul dataset\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signaltonoise(dataset, min, max, axis=0, ddof=0):\n",
    "    \"\"\"Fonction qui détermine le SNR d'un groupe de données.\n",
    "\n",
    "    Args:\n",
    "        datset (dataframe): Données.\n",
    "        min (float) : valeur  minimale pour la normalisation.\n",
    "        max (float) : Valeur maximale pour la normalisation.    \n",
    "        axis (int, optional): Axis along which the means are computed. Default is 0.\n",
    "        ddof (float, int, optional): Delta degrees of freedom. The divisor used in the calculation is N - ddof, where N represents the number of elements.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: Signal to Noise Ratio for each sample of the dataset.\n",
    "    \"\"\"\n",
    "    SNR = []\n",
    "    for df in dataset:\n",
    "        signal = []\n",
    "        subset = df.loc[(df[0] <= min) & (df[0] >= max)]\n",
    "        m = subset[1].mean()\n",
    "        sd = subset[1].std()\n",
    "        noise = np.full(len(df), m)\n",
    "        for i in range(len(df)):\n",
    "            signal += [df[1][i]-noise]\n",
    "        SNR += [signal/noise]\n",
    "    \n",
    "    return SNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moyenne_SNR(dataframe):\n",
    "    \"\"\"Fonction qui fait la moyenne des données des lignes d'un dataframe.\n",
    "\n",
    "    Args:\n",
    "        dataframe (DataFrame): Dataframe de plusieurs colonnes\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame d'une seule colonne de la moyennes de toutes les lignes du dataframe précédent.\n",
    "    \"\"\"\n",
    "    return dataframe.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalisation(dataframe):\n",
    "    \"\"\"Fonction qui normalise les dataframes.\n",
    "\n",
    "    Args:\n",
    "        dataframe (DataFrame)\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Dataframe normalisé.\n",
    "    \"\"\"\n",
    "    return (dataframe-min(dataframe))/np.ptp(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataframes(df_list):\n",
    "    \"\"\"Fonction qui normalise les données d'intensité fournies.\n",
    "\n",
    "    Args:\n",
    "        df_list (list): Liste de dataframes qui contiennent les données à normaliser.\n",
    "\n",
    "    Returns:\n",
    "        list: Liste de dataframes qui contiennent les données normalisées.\n",
    "    \"\"\"\n",
    "    # Trouver le maximum global parmi tous les DataFrame\n",
    "    max_value = -np.inf\n",
    "    for df in df_list:\n",
    "        local_max = np.max(df[1])\n",
    "        if local_max > max_value:\n",
    "            max_value = local_max\n",
    "    \n",
    "    # Normaliser chaque DataFrame sur 1 en utilisant le maximum global\n",
    "    for df in df_list:\n",
    "        df[1] = df[1] / max_value\n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_clean(path):\n",
    "    \"\"\"Fonction qui effectue les trois fonctions suivantes : normalisation, moyenne_SNR et extract_data\n",
    "\n",
    "    Args:\n",
    "        path (path): le path jusqu'au dossier des fichiers à lire.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: dataframe de 1 colonne normalisée.\n",
    "    \"\"\"\n",
    "    return normalisation(moyenne_SNR(extract_data(path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moyenne_dataframes(liste_dataframes):\n",
    "    \"\"\" \n",
    "    Retourne un DataFrame qui contient les moyennes des données de chaque dataframe de la liste.\n",
    "\n",
    "    Args:\n",
    "        - une liste de dataframes : list of pandas DataFrames\n",
    "\n",
    "    Returns:\n",
    "        - un DataFrame contenant les moyennes des données de chaque dataframe, sous la forme suivante :\n",
    "            index   colonne1   colonne2   ...\n",
    "            0       x         y          ...\n",
    "    \"\"\"\n",
    "    # Fusionner les dataframes en une seule dataframe\n",
    "    df_concat = pd.concat(liste_dataframes)\n",
    "\n",
    "    # Regrouper les données par indice et calculer la moyenne pour chaque groupe\n",
    "    donnees_regroupees = df_concat.groupby(df_concat.index)\n",
    "    moyennes = donnees_regroupees.mean()\n",
    "    # Retourner la liste des moyennes\n",
    "    return moyennes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def incertitude_dataframes(liste_dataframes):\n",
    "    \"\"\" \n",
    "    Cette fonction prend en entrée une liste de dataframes et renvoie un dataframe unique qui contient les données des autres dataframes.\n",
    "\n",
    "    Args:\n",
    "        liste_dataframes (list of pandas DataFrame objects) : list of dataframes to be compared.\n",
    "\n",
    "    Returns:\n",
    "        float : the maximum incertitude among all dataframes in the list.\n",
    "    \"\"\"\n",
    "    # Fusionner les dataframes en une seule dataframe\n",
    "    df_concat = pd.concat(liste_dataframes)\n",
    "\n",
    "    # Regrouper les données par indice et calculer l'incertitude pour chaque groupe\n",
    "    donnees_regroupees = df_concat.groupby(df_concat.index)\n",
    "    incertitude = donnees_regroupees[\"Intensity\"].std()\n",
    "    # Retourner la liste des incertitudes\n",
    "    return incertitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moyenne_incertitude_dataframe(liste_dataframe):\n",
    "    \"\"\" \n",
    "    Cette fonction calcule la moyenne incertitude pour un dataframe. \n",
    "\n",
    "    Args:\n",
    "        liste_dataframe (list of pandas DataFrame): La liste des données à traiter.\n",
    "\n",
    "    Returns:\n",
    "        pandas DataFrame : Le dataframe contenant les moyennes et incertitudes calculées.\n",
    "    \"\"\"\n",
    "    moyenne = moyenne_dataframes(liste_dataframe)\n",
    "    incertitude = incertitude_dataframes(liste_dataframe)\n",
    "    return [moyenne, incertitude]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def incertitude_moyenne(liste):\n",
    "    \"\"\" \n",
    "        Calcul de l'incertitude moyenne d'une liste de nombres.\n",
    "\n",
    "    Args:\n",
    "        liste (list of float): La liste des nombres pour lesquels on veut calculer l'incertitude.\n",
    "\n",
    "    Returns:\n",
    "        float : L'incertitude moyenne de la liste.\n",
    "    \"\"\"\n",
    "    moyenne_inc = []\n",
    "    for elem in liste:\n",
    "        moyenne_inc += [elem.mean()]\n",
    "    return moyenne_inc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_intensity(dataset, var, divisor=1):\n",
    "    intensity = []\n",
    "    for df in dataset:\n",
    "        point = df.loc[df[0] == var]\n",
    "        intensity += [np.max(point[1]/divisor)]\n",
    "    return intensity\n",
    "\n",
    "def find_maximum_intensity(dataset,mini=0, maxi=1, divisor=1):\n",
    "    \"\"\"Fonction qui trouve les maximums d'intensité.\n",
    "\n",
    "    Args:\n",
    "        dataset (array): dataset comprenant plusieurs dataframe.\n",
    "\n",
    "    Returns:\n",
    "        array: liste des maximums pour chaque dataframe.\n",
    "    \"\"\"\n",
    "    max_from_dataset = []\n",
    "    for df in dataset:\n",
    "        subset = df.loc[(df[0] >= mini) & (df[0] <= maxi)]\n",
    "        max_from_dataset += [np.max(subset[1]/divisor)]\n",
    "    return max_from_dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
